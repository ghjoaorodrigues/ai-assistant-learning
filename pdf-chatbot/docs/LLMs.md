# Large Language Models

## 1. Definition

LLMs are language models trained on vast amounts of text designed to understand and generate natural language.
They are called 'large' because they work with billions of parameters (values learned during training).
They work by processing text into tokens, which are used to predict the most likely next token based on its learned patterns.

Examples of LLMs include:
- GPT (OpenAI)
- Claude (Anthropic)
- Gemini (Google DeepMind)
- LLaMA (Meta)

## 2. Why it Matters

LLMs are important because they enable the machine to understand and generate human language.

They are the foundation for many modern applications, including:
- Chatbots and virtual assistants (e.g., customer support bots like ChatGPT)
- Text summarization tools (e.g., summarization of long articles or documents)
- Code generation and auto-completion (e.g., GitHub Copilot)

In this project (PDF Chatbot), the LLM is the component that answers users' questions.
It uses the context retrieved from documents to respond accurately to questions while staying relevant to the context of the PDF.

Understanding how LLMs work helps me:
- Choose the right model based on speed, local resource requirements, token limitations, and cost
- Write effective prompts to allow for accurate answers from the LLM
- Recognize limitations like hallucination, lack of memory, and limited reasoning

## 3. Key Concepts

| Term           | Description                                                                                      |
|----------------|--------------------------------------------------------------------------------------------------|
| Token          | Chunks of text (words, subwords, or characters) processed by the LLM one at a time               |
| Prompt         | Input given to the LLM to guide its output                                                       |
| Parameters     | Numerical values learned during the LLM's training which determine its behaviour                 |
| Temperature    | Controls the randomness of text generated by the LLM                                             |
| Context Window | Amount of text, in token format, that a model can consider at any one time                       |
| Hallucination  | Phenomenon where LLMs generate output that is incorrect or inconsistent with their training data |

## 4. How it Works (Simplified)

1. The user types a question or prompt.
2. The LLM breaks the input into tokens.
3. It uses patterns learned during its training to predict and generate the next token(s).
4. The generated tokens are returned as a response.

## 5. Tools and Libraries

You can access or use LLMs via:
- **APIs** (e.g., OpenAI's `gpt-3.5-turbo`)
- **Local Runtimes** (e.g., Mistral with Ollama)
- Python Libraries: `openai`, `transformers`, `llama-cpp-python`

## 6. Sources & References

- [Wikipedia: Large Language Model](https://en.wikipedia.org/wiki/Large_language_model)  
- [IBM: What Are Large Language Models?](https://www.ibm.com/think/topics/large-language-models)  
- [YouTube: Large Language Models Explained Visually](https://www.youtube.com/watch?v=5sLYAQS9sWQ)  
- [YouTube: How Do LLMs Work?](https://www.youtube.com/watch?v=LPZh9BOjkQs)  
- [Medium: What is LLM Tokenization?](https://medium.com/@tahirbalarabe2/what-is-llm-tokenization-a-guide-to-language-model-efficiency-1b4ae57c180b)  
- [Medium: Limitations of Large Language Models](https://medium.com/@marketing_novita.ai/all-you-need-to-know-about-the-limitations-of-large-language-models-568e15f66809)  
- [IBM: What is a Context Window?](https://www.ibm.com/think/topics/context-window)  
- [IBM: LLM Temperature Explained](https://www.ibm.com/think/topics/llm-temperature)
